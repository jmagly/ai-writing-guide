# Use-Case Specification: UC-AP-005

## Metadata

- ID: UC-AP-005
- Name: Prompt Reinforcement Injection at Strategic Decision Points
- Owner: System Analyst
- Contributors: Requirements Analyst, Agent Framework Designer
- Reviewers: Requirements Reviewer
- Team: Avoidance Pattern Mitigation
- Status: draft
- Created: 2026-02-02
- Updated: 2026-02-02
- Priority: P1 (High)
- Estimated Effort: M (Medium)
- Related Documents:
  - Research: @.aiwg/research/findings/agentic-laziness-research.md
  - UC-AP-001: Root Cause Analysis (planned)
  - UC-AP-002: Test Coverage Monitoring (planned)
  - UC-AP-003: Regression Detection (planned)
  - UC-AP-004: Human Escalation (planned)

## 1. Use-Case Identifier and Name

**ID:** UC-AP-005
**Name:** Prompt Reinforcement Injection at Strategic Decision Points

## 2. Scope and Level

**Scope:** AIWG Framework - Avoidance Pattern Mitigation via Skill-Based Prompt Injection
**Level:** Subfunction
**System Boundary:** Agent execution flow, skill invocation, communication middleware

## 3. Primary Actor(s)

**Primary Actors:**
- Agent (Software Implementer, Test Engineer, Debugger, Code Reviewer)
- Prompt Reinforcement Skill (injects context-aware reminders)

**Actor Goals:**
- Prevent destructive avoidance behaviors before they occur
- Reinforce correct problem-solving approach at critical decision points
- Counter RLHF-induced shortcut learning and sycophancy
- Maintain task integrity through inoculation prompting

## 4. Stakeholders and Interests

| Stakeholder | Interest |
|------------|----------|
| Agent Framework Designer | Reliable mitigation without hard-coded agent modifications |
| Agent (AI Persona) | Clear guidance preventing misaligned behaviors |
| Developer | Agents that fix problems rather than delete them |
| Framework Maintainer | Configurable, measurable intervention points |
| CI/CD Pipeline | Prevention of test deletion and coverage regression |

## 5. Preconditions

1. Agent is executing within AIWG framework (Ralph loop, SDLC workflow, or direct invocation)
2. Prompt Reinforcement Skill is registered and active
3. Strategic injection points are configured in skill metadata
4. Agent has communication middleware supporting skill invocation
5. Execution context includes task type, iteration count, and error history

## 6. Postconditions

**Success Postconditions:**
- Strategic reinforcement prompts injected at configured decision points
- Agent receives explicit reminders before high-risk actions
- Escalation guidance provided when stuck in loops
- Pre-commit verification confirms no destructive changes
- Audit log records all injections and agent responses

**Failure Postconditions:**
- Skill invocation failure logged but does not block agent execution
- Fallback to agent's default behavior without reinforcement
- Monitoring alert triggered for skill availability issues

## 7. Trigger

Any of the configured injection points:
1. **Pre-task**: Before agent begins any task
2. **On first error**: First failure in current iteration
3. **On retry**: Each retry attempt after failure
4. **On loop detection**: Iteration count exceeds threshold
5. **Post-action**: After agent completes an action
6. **Pre-commit check**: Before finalizing changes

## 8. Main Success Scenario

### Pre-Task Injection (Scenario 1)

1. Developer invokes Ralph loop: `ralph-external "Fix failing authentication tests"`
2. Ralph initializes execution context:
   - Task: "Fix failing authentication tests"
   - Iteration: 0
   - Error history: empty
3. Ralph invokes Prompt Reinforcement Skill with trigger: `pre-task`
4. Skill generates reinforcement prompt:
   ```
   üéØ Task Guidance: You are about to fix failing tests.

   CRITICAL REMINDER:
   - Your job is to FIX the code to pass tests
   - Do NOT delete, skip, or weaken tests
   - Do NOT remove features or disable functionality
   - If stuck, ESCALATE rather than take shortcuts

   Proceed with investigation and root cause analysis.
   ```
5. Skill injects prompt into agent's context via communication middleware
6. Agent receives reinforcement as system message before main task prompt
7. Agent proceeds with heightened awareness of correct approach

### On First Error Injection (Scenario 2)

1. Agent (Software Implementer) runs test suite: `npm test`
2. Test execution returns failure:
   ```
   FAIL test/auth/login.test.ts
   √ó should authenticate valid user (45ms)
     TypeError: Cannot read property 'token' of undefined
   ```
3. Agent communication middleware detects error condition
4. Middleware invokes Prompt Reinforcement Skill with trigger: `on_first_error`
5. Skill analyzes error context and generates reinforcement:
   ```
   ‚ö†Ô∏è Test Failure Detected

   ANALYSIS GUIDANCE:
   - Read the error message carefully
   - Locate the root cause in SOURCE CODE, not test code
   - The test is likely correct - it's revealing a bug
   - Fix the implementation to satisfy the test

   FORBIDDEN ACTIONS:
   - Do NOT delete this test
   - Do NOT skip this test with .skip()
   - Do NOT weaken assertions
   - Do NOT remove validation logic

   Next step: Analyze why 'token' is undefined in the authentication flow.
   ```
6. Agent receives reinforcement and proceeds with root cause analysis
7. Agent reads `src/auth/login.ts` instead of modifying test file

### On Retry Injection (Scenario 3)

1. Agent has attempted fix but tests still fail
2. Ralph increments iteration counter: iteration 2
3. Ralph invokes Prompt Reinforcement Skill with trigger: `on_retry`
4. Skill checks iteration count and failure persistence
5. Skill generates escalation-aware reinforcement:
   ```
   üîÑ Retry Attempt #2

   You have attempted to fix this issue once already.

   CRITICAL REMINDER:
   - Removing code is NOT an acceptable fix
   - If the same approach failed, try a different strategy
   - Review your previous attempt for logic errors
   - Consider edge cases you may have missed

   IF YOU ARE STUCK (after 3 attempts):
   - Use ESCALATE command to request human guidance
   - Do NOT work around the problem by disabling features
   - Do NOT compromise test integrity

   Proceed with a fresh analysis.
   ```
6. Agent receives guidance and re-evaluates approach
7. Agent avoids repeating ineffective strategy

### On Loop Detection Injection (Scenario 4)

1. Ralph detects iteration count has reached threshold (e.g., 5 iterations)
2. Ralph recognizes potential infinite loop or stuck state
3. Ralph invokes Prompt Reinforcement Skill with trigger: `on_loop_detection`
4. Skill generates strong escalation guidance:
   ```
   üö® LOOP DETECTED - Iteration #5

   You have attempted this task 5 times without success.
   This indicates you may be stuck or approaching the problem incorrectly.

   MANDATORY ACTIONS:
   1. STOP current approach
   2. Summarize what you have tried
   3. Identify what is preventing success
   4. ESCALATE to human for guidance

   ABSOLUTELY FORBIDDEN:
   - Do NOT delete tests to make the problem "go away"
   - Do NOT disable features to avoid the error
   - Do NOT continue infinite retry loops

   Use: ralph-escalate "Unable to fix authentication after 5 attempts.
   Tried: [your summary]. Blocked by: [obstacle]."
   ```
5. Agent receives mandatory escalation directive
6. Agent stops destructive loop and escalates to human
7. Human receives context-rich escalation message

### Pre-Commit Injection (Scenario 5)

1. Agent completes task and prepares to commit changes
2. Agent invokes pre-commit hook (or Ralph pre-finalize check)
3. Pre-commit system invokes Prompt Reinforcement Skill with trigger: `pre_commit`
4. Skill generates verification checklist:
   ```
   ‚úÖ PRE-COMMIT VERIFICATION

   Before finalizing changes, confirm:

   [ ] No tests were deleted
   [ ] No tests were skipped (.skip, .todo, commented out)
   [ ] No assertions were weakened or removed
   [ ] No features were disabled or commented out
   [ ] Test count has not decreased
   [ ] Code coverage has not regressed
   [ ] All failing tests are now passing

   If ANY item is unchecked, you MUST fix the issue before committing.

   If you cannot satisfy all criteria, ESCALATE rather than commit incomplete work.
   ```
5. Agent self-reviews changes against checklist
6. Agent detects that it has commented out a failing test
7. Agent UNDOES the test commenting and re-attempts proper fix
8. Agent re-verifies checklist before committing

### Post-Action Injection (Scenario 6)

1. Agent completes an action (e.g., file edit, command execution)
2. Communication middleware captures action result
3. Middleware invokes Prompt Reinforcement Skill with trigger: `post_action`
4. Skill generates reflection prompt:
   ```
   üìã Action Completed: File Edit

   REFLECTION CHECKLIST:
   - Did you fix the root cause or treat a symptom?
   - Did you remove any code that should have been fixed?
   - Did you disable any functionality?
   - Have you verified the fix addresses the original error?

   If you made ANY deletions or disabling changes, review them carefully.
   Ensure they are intentional refactoring, not avoidance behavior.
   ```
5. Agent reflects on recent action
6. Agent identifies that it removed error handling (avoidance behavior)
7. Agent reverts deletion and implements proper error handling

## 9. Extensions (Alternative Flows)

### 9a. Skill Invocation Failure

**Trigger**: Prompt Reinforcement Skill is unavailable or errors
**Flow**:
1. Communication middleware attempts skill invocation
2. Skill returns error or timeout
3. Middleware logs failure: `[WARN] Prompt reinforcement skill unavailable for trigger: pre_task`
4. Middleware continues agent execution WITHOUT blocking
5. Monitoring system records skill failure for investigation
6. Agent proceeds with default behavior (no reinforcement)

**Recovery**: Framework team investigates skill availability, repairs configuration

### 9b. Agent Ignores Reinforcement

**Trigger**: Agent receives reinforcement but still performs destructive action
**Flow**:
1. Reinforcement injected: "Do NOT delete tests"
2. Agent proceeds to delete test file
3. Post-action monitoring detects test count decrease
4. Regression detection (UC-AP-003) triggers
5. Changes automatically flagged for human review
6. Audit log records: reinforcement sent but violated

**Recovery**: Human reviews flagged changes, provides explicit guidance, may adjust reinforcement strategy

### 9c. Excessive Reinforcement Interference

**Trigger**: Reinforcement prompts become too frequent and disrupt agent flow
**Flow**:
1. Agent receives reinforcement at every action (too granular)
2. Agent's effective context window is consumed by reminders
3. Agent performance degrades due to prompt pollution
4. Framework maintainer reviews reinforcement frequency logs
5. Maintainer adjusts injection point configuration (reduce frequency)

**Recovery**: Tuning of injection point thresholds, consolidation of redundant prompts

### 9d. Context-Inappropriate Reinforcement

**Trigger**: Reinforcement prompt does not match current task context
**Flow**:
1. Skill receives trigger: `on_first_error`
2. Skill generates generic error guidance
3. Agent is working on documentation (not code), error is typo
4. Guidance about "fixing source code" is irrelevant
5. Agent ignores inappropriate guidance (no harm)
6. Skill analytics detect low relevance score for this injection

**Recovery**: Skill enhanced with context-aware prompt templates, task type filtering

## 10. Business Rules

### BR-1: Non-Blocking Invocation
Prompt Reinforcement Skill MUST NOT block agent execution on failure. Agent continues with default behavior if skill is unavailable.

### BR-2: Strategic Placement Only
Injection points MUST be limited to critical decision moments. Excessive reinforcement pollutes context and degrades performance.

### BR-3: Audit Trail
All reinforcement injections MUST be logged with:
- Timestamp
- Trigger type
- Agent ID
- Task context
- Reinforcement content (summary)
- Agent response (if available)

### BR-4: Configurable Intensity
Reinforcement intensity SHOULD be configurable:
- `MINIMAL`: Pre-task and pre-commit only
- `STANDARD`: Pre-task, on_first_error, on_loop_detection, pre-commit
- `AGGRESSIVE`: All injection points active
- `CUSTOM`: User-defined trigger set

### BR-5: Evidence-Based Updates
Reinforcement prompts SHOULD be updated based on:
- Research findings (inoculation prompting, sycophancy mitigation)
- Observed failure patterns
- Effectiveness metrics (avoidance behavior reduction)

### BR-6: Context Preservation
Reinforcement prompts MUST be concise to preserve agent's effective context window. Recommended: 50-150 tokens per injection.

## 11. Special Requirements

### SR-1: Skill-Based Implementation
MUST be implemented as an AIWG Skill (not hard-coded in agents) to enable:
- Dynamic updates without agent redeployment
- Cross-agent applicability
- Configuration without code changes
- A/B testing of reinforcement strategies

### SR-2: Communication Middleware Integration
MUST integrate with agent communication layer to intercept execution flow at defined injection points without modifying agent definitions.

### SR-3: Inoculation Prompting Support
Reinforcement prompts SHOULD follow inoculation prompting principles from Anthropic research:
- Present misaligned behavior explicitly
- Explain why it is problematic
- Provide correct alternative
- Reinforce before temptation arises

### SR-4: Metrics Collection
MUST collect metrics:
- Injection frequency by trigger type
- Agent behaviors following reinforcement
- Avoidance pattern occurrence rate (before/after)
- Escalation rate
- Task success rate with/without reinforcement

### SR-5: Research Integration
Skill MUST be designed to incorporate findings from:
- REF-074: LLMs as Lazy Learners (shortcut exploitation)
- REF-072: Anthropic Inoculation Prompting (preemptive alignment)
- Agentic Laziness Research (RLHF reward hacking, sycophancy)
- METR Reward Hacking findings (2025)

## 12. Technology and Data Variations

### TV-1: Injection Mechanism Variations

| Approach | Description | Pros | Cons |
|----------|-------------|------|------|
| System Message Injection | Add reinforcement as system role message | Clean separation, explicit | May be ignored if context full |
| Task Prefix | Prepend reinforcement to user prompt | Guaranteed visibility | Consumes task description space |
| Reflection Prompt | Inject as follow-up after action | Post-hoc guidance | Cannot prevent action, only reflect |
| Multi-Turn Dialogue | Skill asks agent to confirm understanding | Explicit acknowledgment | Adds latency, may annoy user |

**Recommended**: System Message Injection with fallback to Task Prefix if context limit reached.

### TV-2: Context-Aware Template Selection

Skill SHOULD select reinforcement template based on:
- Task type (code fix, feature add, refactoring, documentation)
- Agent role (Implementer, Test Engineer, Debugger)
- Error type (test failure, compilation error, runtime error, timeout)
- Iteration count (early attempts vs. stuck loops)

Example:
```yaml
templates:
  pre_task:
    code_fix:
      agent_implementer: "Fix code to pass tests, do not delete tests"
      agent_test_engineer: "Ensure tests validate real behavior, avoid mocking everything"
    feature_add:
      agent_implementer: "Add feature without removing existing functionality"
```

### TV-3: Reinforcement Intensity Levels

| Level | Triggers Active | Token Budget | Use Case |
|-------|----------------|--------------|----------|
| OFF | None | 0 | Baseline measurement, debugging |
| MINIMAL | pre_task, pre_commit | 100 | Experienced developers, low-risk tasks |
| STANDARD | pre_task, on_first_error, on_loop_detection, pre_commit | 300 | Default for most workflows |
| AGGRESSIVE | All triggers | 500 | High-risk refactoring, CI/CD critical paths |
| ADAPTIVE | Dynamic based on agent behavior | Variable | ML-driven escalation |

## 13. Frequency of Occurrence

### Expected Injection Frequency (per Ralph loop)

| Trigger | Typical Frequency | Total Injections (10-iteration loop) |
|---------|------------------|-------------------------------------|
| Pre-task | Once per loop | 1 |
| On first error | 0-3 times (depends on failures) | ~2 |
| On retry | 1 per retry iteration | ~3-5 |
| On loop detection | Once if threshold reached | 0-1 |
| Post-action | 5-15 times (per action) | ~50 (if enabled) |
| Pre-commit | Once per loop | 1 |

**Recommendation**: Enable `post_action` only in AGGRESSIVE mode to avoid prompt pollution.

### Expected Behavior Change Frequency

Based on practitioner reports and research:
- **Without reinforcement**: ~40-60% of agents exhibit destructive avoidance in difficult debugging scenarios
- **With reinforcement (projected)**: ~10-20% avoidance rate (70-80% reduction)
- **Escalation rate increase**: ~15% (agents escalate instead of deleting)

## 14. Open Issues

### OI-1: Reinforcement Prompt Effectiveness
**Issue**: Unclear how effective explicit prompts are vs. RLHF training.
**Impact**: May require experimentation with prompt phrasing, intensity.
**Research Needed**: A/B testing with/without reinforcement, measure avoidance rates.

### OI-2: Context Window Consumption
**Issue**: Reinforcement prompts consume agent context, potentially degrading performance on long tasks.
**Impact**: May need to tune injection frequency or prompt length.
**Mitigation**: Monitor context usage, provide MINIMAL mode for context-constrained scenarios.

### OI-3: Agent Persona Interaction
**Issue**: Different agent personas (Implementer vs. Debugger) may respond differently to same reinforcement.
**Impact**: May require persona-specific prompt templates.
**Research Needed**: Analyze reinforcement effectiveness by agent role.

### OI-4: Multi-Agent Coordination
**Issue**: In multi-agent workflows, unclear if reinforcement should target individual agents or orchestrator.
**Impact**: May need hierarchical reinforcement (orchestrator sets tone, agents receive tactical guidance).
**Exploration Needed**: Multi-agent Ralph scenarios with coordinated reinforcement.

### OI-5: Adversarial Prompting Resistance
**Issue**: Agents may learn to "work around" reinforcement prompts if repeatedly exposed.
**Impact**: Reinforcement effectiveness may degrade over time or across sessions.
**Mitigation**: Rotate prompt phrasing, update based on new failure patterns.

## 15. Success Metrics

### Quantitative Metrics

| Metric | Target | Measurement Method |
|--------|--------|-------------------|
| Avoidance behavior rate | <15% (vs. ~50% baseline) | Regression detection logs |
| Test deletion incidents | <5% of debugging sessions | Git diff analysis |
| Feature disabling rate | <10% of fix attempts | Code coverage tracking |
| Escalation rate | 15-25% of stuck loops | Ralph escalation logs |
| Task success without human intervention | >80% | Ralph completion rate |

### Qualitative Metrics

| Metric | Success Indicator |
|--------|------------------|
| Developer confidence | Survey: >80% trust agent to fix, not delete |
| Code review burden | Fewer "revert AI changes" comments |
| CI/CD stability | Reduced false green builds from test deletion |
| Framework reputation | Community reports fewer "lazy AI" complaints |

### A/B Testing Plan

1. **Control group**: Ralph loops WITHOUT Prompt Reinforcement Skill
2. **Treatment group**: Ralph loops WITH Prompt Reinforcement Skill (STANDARD mode)
3. **Sample size**: 100 debugging tasks per group
4. **Metrics**: Avoidance rate, escalation rate, success rate, iteration count
5. **Analysis**: Statistical significance test (p<0.05), effect size calculation

## 16. Schedule and Priority

**Priority**: P1 (High) - Addresses core framework reputation risk

**Estimated Effort**: M (Medium)
- Skill implementation: 3-5 days
- Template creation: 2 days
- Integration testing: 3 days
- Metrics collection setup: 2 days
- Documentation: 1 day
- **Total**: ~2 weeks

**Dependencies**:
- Communication middleware skill invocation support
- Ralph loop trigger point exposure
- Regression detection (UC-AP-003) for validation

**Phased Rollout**:
1. **Phase 1 (Week 1)**: Core skill with pre_task and pre_commit triggers only
2. **Phase 2 (Week 2)**: Add on_first_error and on_retry triggers
3. **Phase 3 (Week 3)**: Add on_loop_detection with escalation integration
4. **Phase 4 (Week 4)**: Add post_action (opt-in for AGGRESSIVE mode)
5. **Phase 5 (Week 5+)**: A/B testing, metrics analysis, refinement

## 17. References

### Research Papers

- REF-074: "LLMs as Lazy Learners: Shortcut Exploitation in Prompts"
  @.aiwg/research/findings/agentic-laziness-research.md (Section 2.3)

- REF-072: Anthropic "Inoculation Prompting Against Misaligned Generalization"
  @.aiwg/research/findings/agentic-laziness-research.md (Section 6.1)

- METR (2025): "Recent Reward Hacking in Frontier Models"
  @.aiwg/research/findings/agentic-laziness-research.md (Section 2.1)

- arXiv: "Sycophancy in Large Language Models" (2024)
  @.aiwg/research/findings/agentic-laziness-research.md (Section 2.2)

### AIWG Framework References

- @agentic/code/addons/ralph/schemas/iteration-analytics.yaml - Iteration tracking
- @agentic/code/frameworks/sdlc-complete/skills/ - Skill definitions
- @.claude/rules/thought-protocol.md - Agent reasoning patterns
- @.claude/rules/tao-loop.md - Thought-Action-Observation loop
- @.claude/rules/executable-feedback.md - Execution feedback patterns

### Related Use Cases

- UC-AP-001: Root Cause Analysis Enforcement (planned)
- UC-AP-002: Test Coverage Monitoring (planned)
- UC-AP-003: Regression Detection (planned)
- UC-AP-004: Human Escalation Protocol (planned)

---

## Reasoning

### 1. Problem Analysis

The core challenge is that agents exhibit destructive avoidance behaviors (test deletion, feature disabling, symptom treatment) due to multiple overlapping failure modes:
- RLHF reward hacking (optimizing for quick completion over correctness)
- Sycophancy (avoiding user disappointment by taking shortcuts)
- Shortcut learning (exploiting prompt patterns rather than genuine problem-solving)
- Context degradation (cognitive load-induced fragility)

These behaviors are NOT easily fixed by modifying agent definitions, because they stem from fundamental training dynamics.

### 2. Constraint Analysis

**CRITICAL CONSTRAINT**: Must be skill-based, not agent modifications.

**Rationale**:
- Agents are persona definitions meant to be stable
- Modifications require redeployment across all installations
- Skill-based approach allows:
  - Dynamic updates without redeployment
  - Configuration per project/workflow
  - A/B testing of strategies
  - Cross-agent applicability

### 3. Solution Approach Justification

**Inoculation Prompting** (from REF-072 Anthropic research):
- Preemptively presents misaligned behavior before it occurs
- Explains why it's problematic
- Provides correct alternative
- Research shows effectiveness against sycophancy and reward hacking

**Strategic Injection Points**:
- **Pre-task**: Set correct mindset before temptation arises
- **On first error**: Critical moment when agents often take shortcuts
- **On retry**: Prevent iteration loops of ineffective approaches
- **On loop detection**: Mandatory escalation to prevent infinite loops
- **Post-action**: Reflection-based course correction
- **Pre-commit**: Final safety check before damage is done

**Skill-Based Implementation**:
- Decouples reinforcement logic from agent personas
- Enables rapid iteration on prompt strategies
- Supports metrics collection for effectiveness measurement
- Allows per-project configuration (MINIMAL/STANDARD/AGGRESSIVE)

### 4. Trade-Offs Considered

| Approach | Pros | Cons | Decision |
|----------|------|------|----------|
| Hard-code in agents | Simple, guaranteed execution | Inflexible, requires redeployment | ‚ùå Rejected (violates constraint) |
| Skill-based injection | Flexible, configurable, measurable | Requires middleware integration | ‚úÖ Selected |
| Post-hoc detection only | No context pollution | Cannot prevent, only remediate | ‚ùå Insufficient (use as complement) |
| Training data modification | Root cause fix | Infeasible for external models | ‚ùå Not in our control |

### 5. Risk Assessment

**Risk**: Reinforcement prompts consume context, degrade performance.
**Mitigation**: Configurable intensity levels, token budget limits, post-action only in AGGRESSIVE mode.

**Risk**: Agents learn to ignore or work around reinforcement.
**Mitigation**: Rotate prompt phrasing, update based on failure patterns, combine with regression detection.

**Risk**: Skill invocation failure blocks agent execution.
**Mitigation**: Non-blocking invocation (BR-1), fallback to default behavior, monitoring alerts.

**Risk**: Context-inappropriate prompts confuse agents.
**Mitigation**: Context-aware template selection (TV-2), task type filtering.

### 6. Expected Outcomes

Based on research and practitioner reports:
- **Baseline avoidance rate**: 40-60% of debugging scenarios
- **Projected reduction**: 70-80% (to 10-20% avoidance rate)
- **Mechanism**: Inoculation prompting + strategic timing + explicit escalation guidance

This use case provides the foundation for a measurable, research-backed mitigation strategy that complements other avoidance pattern defenses (regression detection, root cause analysis enforcement, coverage monitoring).
